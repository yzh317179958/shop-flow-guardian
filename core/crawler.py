"""
Fiido ç”µå•†ç½‘ç«™äº§å“çˆ¬è™«æ¨¡å—

è´Ÿè´£ä» Fiido.com ç½‘ç«™å‘ç°å’ŒæŠ“å–å•†å“ä¿¡æ¯ï¼Œæ”¯æŒ Shopify JSON API å’Œ HTML è§£æä¸¤ç§æ–¹å¼ã€‚
é›†æˆç¼“å­˜æœºåˆ¶ï¼Œæå‡æ€§èƒ½ã€‚
"""

import logging
import time
from typing import List, Optional, Dict, Any
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from core.models import Product, ProductVariant, Selectors
from core.cache import CrawlerCache

logger = logging.getLogger(__name__)


class ProductCrawler:
    """Fiido ç½‘ç«™äº§å“çˆ¬è™«

    å®ç°å•†å“åˆ†ç±»å‘ç°ã€å•†å“åˆ—è¡¨æŠ“å–ã€å•†å“è¯¦æƒ…è§£æç­‰åŠŸèƒ½ã€‚
    ä¼˜å…ˆä½¿ç”¨ Shopify JSON APIï¼Œå¤±è´¥æ—¶é™çº§åˆ° HTML è§£æã€‚
    æ”¯æŒç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤çˆ¬å–ã€‚
    """

    def __init__(
        self,
        base_url: str = "https://fiido.com",
        timeout: int = 30,
        max_retries: int = 3,
        use_cache: bool = True,
        cache_ttl_hours: int = 24
    ):
        """åˆå§‹åŒ–äº§å“çˆ¬è™«

        Args:
            base_url: ç½‘ç«™æ ¹ URL
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            use_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_ttl_hours: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰
        """
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.use_cache = use_cache

        # åˆå§‹åŒ–ç¼“å­˜
        if use_cache:
            self.cache = CrawlerCache(ttl_hours=cache_ttl_hours)
            print(f"âœ… ç¼“å­˜å·²å¯ç”¨ï¼ˆTTL: {cache_ttl_hours}å°æ—¶ï¼‰")
        else:
            self.cache = None

        # é…ç½®å¸¦é‡è¯•æœºåˆ¶çš„ Session
        self.session = requests.Session()
        retry_strategy = Retry(
            total=max_retries,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"],
            backoff_factor=1
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })

        logger.info(f"ProductCrawler initialized for {base_url}")

    def discover_collections(self) -> List[str]:
        """å‘ç°æ‰€æœ‰å•†å“åˆ†ç±»

        ä¼˜å…ˆä»å®˜ç½‘ä¸»é¡µå‘ç°åˆ†ç±»ï¼Œå¤±è´¥æ—¶å°è¯• /collections é¡µé¢ã€‚
        ä¼šè‡ªåŠ¨å»é™¤é‡å¤åˆ†ç±»ã€‚

        Returns:
            åˆ†ç±» URL åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š['/collections/bikes', '/collections/accessories']

        Raises:
            requests.RequestException: ç½‘ç»œè¯·æ±‚å¤±è´¥
        """
        collection_links = set()

        # ç­–ç•¥1: ä»ä¸»é¡µå‘ç°åˆ†ç±»ï¼ˆä¼˜å…ˆï¼‰
        try:
            logger.info(f"Discovering collections from homepage: {self.base_url}")
            homepage_collections = self._discover_collections_from_page(self.base_url)
            collection_links.update(homepage_collections)
            logger.info(f"Found {len(homepage_collections)} collections from homepage")
        except Exception as e:
            logger.warning(f"Failed to discover from homepage: {e}")

        # ç­–ç•¥2: ä» /collections é¡µé¢å‘ç°ï¼ˆè¡¥å……ï¼‰
        try:
            collections_url = f"{self.base_url}/collections"
            logger.info(f"Discovering collections from: {collections_url}")
            collections_page = self._discover_collections_from_page(collections_url)
            collection_links.update(collections_page)
            logger.info(f"Found {len(collections_page)} collections from /collections page")
        except Exception as e:
            logger.warning(f"Failed to discover from /collections page: {e}")

        if not collection_links:
            logger.error("No collections found! Please check the website structure.")
            raise ValueError("No collections found")

        result = sorted(list(collection_links))
        logger.info(f"ğŸ“Š Total unique collections discovered: {len(result)}")
        logger.info(f"Collections: {result}")
        return result

    def _discover_collections_from_page(self, url: str) -> set:
        """ä»æŒ‡å®šé¡µé¢å‘ç°å•†å“åˆ†ç±»é“¾æ¥

        Args:
            url: è¦æ‰«æçš„é¡µé¢URL

        Returns:
            åˆ†ç±»URLé›†åˆ
        """
        response = self.session.get(url, timeout=self.timeout)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        collection_links = set()

        # ç­–ç•¥1: æŸ¥æ‰¾åŒ…å« /collections/ çš„æ‰€æœ‰é“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href']
            if '/collections/' in href and href != '/collections' and href != '/collections/':
                # æ ‡å‡†åŒ– URL
                if href.startswith('http'):
                    # ç»å¯¹ URLï¼Œæå–è·¯å¾„éƒ¨åˆ†
                    if self.base_url in href:
                        href = href.split(self.base_url)[1]
                    else:
                        continue
                elif not href.startswith('/'):
                    # ç›¸å¯¹è·¯å¾„ï¼Œæ·»åŠ å‰å¯¼æ–œæ 
                    href = '/' + href

                # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
                href = href.split('?')[0].split('#')[0]

                # è¿‡æ»¤æ‰æ— æ•ˆçš„è·¯å¾„
                if self._is_valid_collection_path(href):
                    collection_links.add(href)

        # ç­–ç•¥2: æŸ¥æ‰¾å¯¼èˆªèœå•ä¸­çš„åˆ†ç±»é“¾æ¥ï¼ˆShopify å¸¸è§æ¨¡å¼ï¼‰
        nav_selectors = [
            '.site-nav a[href*="/collections/"]',
            '.menu a[href*="/collections/"]',
            '.navigation a[href*="/collections/"]',
            'nav a[href*="/collections/"]',
            'header a[href*="/collections/"]',
            '.header a[href*="/collections/"]'
        ]

        for selector in nav_selectors:
            for link in soup.select(selector):
                href = link.get('href', '')
                if href and '/collections/' in href:
                    href = href.split('?')[0].split('#')[0]
                    if not href.startswith('http') and self._is_valid_collection_path(href):
                        collection_links.add(href)

        return collection_links

    def _is_valid_collection_path(self, path: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„åˆ†ç±»è·¯å¾„

        Args:
            path: URLè·¯å¾„

        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        # æ’é™¤ä¸€äº›å·²çŸ¥çš„éåˆ†ç±»è·¯å¾„
        invalid_patterns = [
            '/collections/all',
            '/collections/vendors',
            '/collections/types',
            '/account',
            '/cart',
            '/checkout',
            '/search',
            '/pages/'
        ]

        for pattern in invalid_patterns:
            if pattern in path:
                return False

        # å¿…é¡»ä»¥ /collections/ å¼€å¤´ä¸”åé¢æœ‰å†…å®¹
        if not path.startswith('/collections/'):
            return False

        # æå–åˆ†ç±»åç§°éƒ¨åˆ†
        collection_name = path.replace('/collections/', '')
        if not collection_name or collection_name == '':
            return False

        return True

    def _format_category_name(self, category_slug: str) -> str:
        """æ ¼å¼åŒ–åˆ†ç±»åç§°ï¼Œå°†URL slugè½¬æ¢ä¸ºå‹å¥½çš„æ˜¾ç¤ºåç§°

        Args:
            category_slug: URLä¸­çš„åˆ†ç±»åç§°ï¼Œå¦‚ 'electric-bikes'

        Returns:
            æ ¼å¼åŒ–åçš„åˆ†ç±»åç§°ï¼Œå¦‚ 'Electric Bikes'
        """
        # æ›¿æ¢è¿å­—ç¬¦ä¸ºç©ºæ ¼
        category = category_slug.replace('-', ' ').replace('_', ' ')

        # é¦–å­—æ¯å¤§å†™ï¼ˆTitle Caseï¼‰
        category = category.title()

        # å¤„ç†ä¸€äº›ç‰¹æ®Šçš„ç¼©å†™è¯ï¼Œä¿æŒå¤§å†™
        special_words = {
            'Ebike': 'eBike',
            'Ebikes': 'eBikes',
            'E Bike': 'E-Bike',
            'E Bikes': 'E-Bikes'
        }

        for old, new in special_words.items():
            category = category.replace(old, new)

        return category

    def discover_products(
        self,
        collection_path: str,
        limit: Optional[int] = None
    ) -> List[Product]:
        """ä»æŒ‡å®šåˆ†ç±»ä¸­å‘ç°æ‰€æœ‰å•†å“

        ä¼˜å…ˆä½¿ç”¨ Shopify JSON APIï¼ˆ.json åç¼€ï¼‰ï¼Œå¤±è´¥æ—¶é™çº§åˆ° HTML è§£æã€‚

        Args:
            collection_path: åˆ†ç±»è·¯å¾„ï¼Œä¾‹å¦‚ '/collections/bikes'
            limit: é™åˆ¶æŠ“å–å•†å“æ•°é‡ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶

        Returns:
            Product å¯¹è±¡åˆ—è¡¨

        Raises:
            requests.RequestException: ç½‘ç»œè¯·æ±‚å¤±è´¥
        """
        logger.info(f"Discovering products from {collection_path} (limit: {limit})")

        # ä¼˜å…ˆå°è¯• Shopify JSON API
        try:
            products = self._discover_products_via_json(collection_path, limit)
            if products:
                logger.info(f"Discovered {len(products)} products via JSON API")
                return products
        except Exception as e:
            logger.warning(f"JSON API failed, falling back to HTML parsing: {e}")

        # é™çº§åˆ° HTML è§£æ
        try:
            products = self._discover_products_via_html(collection_path, limit)
            logger.info(f"Discovered {len(products)} products via HTML parsing")
            return products
        except Exception as e:
            logger.error(f"Failed to discover products: {e}")
            raise

    def _discover_products_via_json(
        self,
        collection_path: str,
        limit: Optional[int] = None
    ) -> List[Product]:
        """é€šè¿‡ Shopify JSON API å‘ç°å•†å“

        Args:
            collection_path: åˆ†ç±»è·¯å¾„
            limit: é™åˆ¶æŠ“å–å•†å“æ•°é‡

        Returns:
            Product å¯¹è±¡åˆ—è¡¨
        """
        # æ„å»º JSON API URL
        json_url = f"{self.base_url}{collection_path}.json"
        products = []
        page = 1

        while True:
            # Shopify JSON API åˆ†é¡µå‚æ•°
            params = {'page': page}
            if limit and len(products) >= limit:
                break

            logger.debug(f"Fetching {json_url}?page={page}")
            response = self.session.get(json_url, params=params, timeout=self.timeout)
            response.raise_for_status()

            data = response.json()

            # Shopify è¿”å›æ ¼å¼: {"collection": {...}, "products": [...]}
            if 'products' not in data or not data['products']:
                break

            for product_data in data['products']:
                if limit and len(products) >= limit:
                    break

                product = self._parse_product_from_json(product_data, collection_path)
                if product:
                    products.append(product)

            # å¦‚æœè¿”å›å•†å“æ•°é‡å°‘äºé¢„æœŸï¼Œè¯´æ˜å·²åˆ°æœ€åä¸€é¡µ
            if len(data['products']) < 30:  # Shopify é»˜è®¤æ¯é¡µ 30 ä¸ªå•†å“
                break

            page += 1
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«

        return products

    def _parse_product_from_json(
        self,
        product_data: Dict[str, Any],
        collection_path: str
    ) -> Optional[Product]:
        """ä» Shopify JSON æ•°æ®è§£æå•†å“ä¿¡æ¯

        Args:
            product_data: Shopify äº§å“ JSON æ•°æ®
            collection_path: æ‰€å±åˆ†ç±»è·¯å¾„

        Returns:
            Product å¯¹è±¡ï¼Œè§£æå¤±è´¥è¿”å› None
        """
        try:
            # æ„å»ºå•†å“ URL
            handle = product_data.get('handle', '')
            product_url = f"{self.base_url}/products/{handle}"

            # æ£€æŸ¥ç¼“å­˜
            if self.use_cache and self.cache:
                cached_data = self.cache.get(product_url)
                if cached_data:
                    try:
                        return Product(**cached_data)
                    except Exception as e:
                        logger.warning(f"Failed to load from cache: {e}")

            # æå–åŸºæœ¬ä¿¡æ¯
            product_id = str(product_data.get('id', ''))

            # æå–ä»·æ ¼èŒƒå›´
            variants_data = product_data.get('variants', [])
            prices = [float(v.get('price', 0)) for v in variants_data if v.get('price')]

            if not prices:
                logger.warning(f"Product {handle} has no valid prices, skipping")
                return None

            price_min = min(prices)
            price_max = max(prices)

            # æå–å˜ä½“ä¿¡æ¯
            variants = []
            for variant_data in variants_data:
                variant = self._parse_variant_from_json(variant_data)
                if variant:
                    variants.append(variant)

            # æå–åˆ†ç±»åç§° - ä¿ç•™åŸå§‹å®˜ç½‘åˆ†ç±»
            # ä» collection_path æå–ï¼Œä¾‹å¦‚ '/collections/electric-bikes' -> 'Electric Bikes'
            category_slug = collection_path.split('/')[-1]
            category = self._format_category_name(category_slug)

            # æå–æ ‡ç­¾
            tags = product_data.get('tags', [])
            if isinstance(tags, str):
                tags = [t.strip() for t in tags.split(',')]

            # åˆ›å»ºé»˜è®¤é€‰æ‹©å™¨ï¼ˆå°†åœ¨è¿è¡Œæ—¶ç”± SelectorManager æ›´æ–°ï¼‰
            selectors = Selectors()

            product = Product(
                id=product_id,
                name=product_data.get('title', ''),
                url=product_url,
                category=category,
                price_min=price_min,
                price_max=price_max,
                currency="USD",
                variants=variants,
                selectors=selectors,
                priority="P1",
                tags=tags,
                metadata={
                    'vendor': product_data.get('vendor', ''),
                    'product_type': product_data.get('product_type', ''),
                    'handle': handle,
                    'available': product_data.get('available', False),
                    'collection_path': collection_path,  # ä¿ç•™åŸå§‹åˆ†ç±»è·¯å¾„
                    'category_slug': category_slug  # ä¿ç•™URLå‹å¥½çš„åˆ†ç±»åç§°
                }
            )

            # ä¿å­˜åˆ°ç¼“å­˜
            if self.use_cache and self.cache:
                self.cache.set(product_url, product.model_dump(mode='json'))

            return product

        except Exception as e:
            logger.error(f"Failed to parse product from JSON: {e}")
            return None

    def _parse_variant_from_json(
        self,
        variant_data: Dict[str, Any]
    ) -> Optional[ProductVariant]:
        """ä» Shopify JSON æ•°æ®è§£æå•†å“å˜ä½“

        Args:
            variant_data: Shopify å˜ä½“ JSON æ•°æ®

        Returns:
            ProductVariant å¯¹è±¡ï¼Œè§£æå¤±è´¥è¿”å› None
        """
        try:
            # Shopify å˜ä½“ç»“æ„: option1, option2, option3
            options = []
            for i in range(1, 4):
                option = variant_data.get(f'option{i}')
                if option and option.lower() not in ['default title', 'default']:
                    options.append(option)

            if not options:
                return None

            # æ¨æ–­å˜ä½“ç±»å‹
            variant_name = ' / '.join(options)
            variant_type = self._infer_variant_type(variant_name)

            # ç”Ÿæˆé€‰æ‹©å™¨ï¼ˆå°†åœ¨è¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´ï¼‰
            selector = f"[data-variant-id='{variant_data.get('id')}']"

            return ProductVariant(
                name=variant_name,
                type=variant_type,
                selector=selector,
                available=variant_data.get('available', False),
                price_modifier=None  # ä»·æ ¼å·®å¼‚åœ¨ Product ä¸­å·²ä½“ç°
            )

        except Exception as e:
            logger.error(f"Failed to parse variant from JSON: {e}")
            return None

    def _infer_variant_type(self, variant_name: str) -> str:
        """æ¨æ–­å˜ä½“ç±»å‹

        Args:
            variant_name: å˜ä½“åç§°ï¼Œå¦‚ 'Black', 'Large', 'Sport'

        Returns:
            å˜ä½“ç±»å‹: 'color', 'size', 'style', 'configuration'
        """
        name_lower = variant_name.lower()

        # é¢œè‰²å…³é”®è¯
        color_keywords = ['black', 'white', 'red', 'blue', 'green', 'yellow', 'gray', 'silver']
        if any(keyword in name_lower for keyword in color_keywords):
            return 'color'

        # å°ºå¯¸å…³é”®è¯
        size_keywords = ['small', 'medium', 'large', 'xs', 's', 'm', 'l', 'xl', 'xxl']
        if any(keyword == name_lower or keyword in name_lower.split() for keyword in size_keywords):
            return 'size'

        # é…ç½®å…³é”®è¯
        config_keywords = ['standard', 'pro', 'plus', 'premium', 'basic', 'advanced']
        if any(keyword in name_lower for keyword in config_keywords):
            return 'configuration'

        # é»˜è®¤ä¸ºæ ·å¼
        return 'style'

    def _discover_products_via_html(
        self,
        collection_path: str,
        limit: Optional[int] = None
    ) -> List[Product]:
        """é€šè¿‡ HTML è§£æå‘ç°å•†å“ï¼ˆé™çº§æ–¹æ¡ˆï¼‰

        Args:
            collection_path: åˆ†ç±»è·¯å¾„
            limit: é™åˆ¶æŠ“å–å•†å“æ•°é‡

        Returns:
            Product å¯¹è±¡åˆ—è¡¨
        """
        collection_url = f"{self.base_url}{collection_path}"
        response = self.session.get(collection_url, timeout=self.timeout)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        products = []

        # æŸ¥æ‰¾å•†å“é“¾æ¥ï¼ˆShopify å¸¸è§æ¨¡å¼ï¼‰
        product_selectors = [
            '.product-item a[href*="/products/"]',
            '.product-card a[href*="/products/"]',
            '.grid-item a[href*="/products/"]',
            'a.product-link[href*="/products/"]',
            'a[href*="/products/"]'
        ]

        product_links = set()
        for selector in product_selectors:
            for link in soup.select(selector):
                href = link.get('href', '')
                if '/products/' in href:
                    # æ ‡å‡†åŒ– URL
                    if not href.startswith('http'):
                        href = urljoin(self.base_url, href)

                    # ç§»é™¤æŸ¥è¯¢å‚æ•°
                    href = href.split('?')[0]
                    product_links.add(href)

                    if limit and len(product_links) >= limit:
                        break

            if limit and len(product_links) >= limit:
                break

        # æå–åˆ†ç±»åç§°
        category = collection_path.split('/')[-1].replace('-', ' ').title()

        # ä¸ºæ¯ä¸ªå•†å“é“¾æ¥åˆ›å»ºåŸºæœ¬ Product å¯¹è±¡
        for product_url in sorted(product_links):
            if limit and len(products) >= limit:
                break

            # ä» URL æå– handle
            handle = product_url.split('/products/')[-1].split('?')[0]

            # åˆ›å»ºåŸºæœ¬å•†å“å¯¹è±¡ï¼ˆè¯¦ç»†ä¿¡æ¯éœ€è¦è¿›ä¸€æ­¥æŠ“å–ï¼‰
            product = Product(
                id=handle,
                name=handle.replace('-', ' ').title(),
                url=product_url,
                category=category,
                price_min=0.0,  # éœ€è¦è¿›ä¸€æ­¥æŠ“å–
                price_max=0.0,
                currency="USD",
                variants=[],
                selectors=Selectors(),
                priority="P1",
                tags=[],
                metadata={'handle': handle, 'needs_detail_fetch': True}
            )

            products.append(product)

        logger.info(f"Found {len(products)} product links via HTML parsing")
        return products

    def close(self):
        """å…³é—­ Sessionï¼Œé‡Šæ”¾èµ„æº"""
        self.session.close()
        logger.info("ProductCrawler session closed")
