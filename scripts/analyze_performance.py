#!/usr/bin/env python3
"""
æµ‹è¯•æ€§èƒ½åˆ†æå·¥å…·

åˆ†ææµ‹è¯•æ‰§è¡Œæ—¶é—´ï¼Œè¯†åˆ«æ€§èƒ½ç“¶é¢ˆï¼Œç”Ÿæˆä¼˜åŒ–å»ºè®®ã€‚
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
from datetime import datetime
import argparse


class PerformanceAnalyzer:
    """æµ‹è¯•æ€§èƒ½åˆ†æå™¨"""

    def __init__(self, results_file: str = "reports/test-results.json"):
        self.results_file = Path(results_file)
        self.results = self._load_results()

    def _load_results(self) -> Dict:
        """åŠ è½½æµ‹è¯•ç»“æœ"""
        if not self.results_file.exists():
            print(f"âŒ æµ‹è¯•ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {self.results_file}")
            return {}

        with open(self.results_file) as f:
            return json.load(f)

    def analyze(self) -> Dict:
        """
        åˆ†ææµ‹è¯•æ€§èƒ½

        Returns:
            æ€§èƒ½åˆ†ææŠ¥å‘Š
        """
        if not self.results:
            return {
                "status": "no_data",
                "message": "æ— æµ‹è¯•æ•°æ®"
            }

        report = {
            "timestamp": datetime.now().isoformat(),
            "total_duration": self.results.get('duration', 0),
            "total_tests": self.results.get('total', 0),
            "avg_test_duration": 0,
            "slowest_tests": [],
            "performance_score": 0,
            "bottlenecks": [],
            "recommendations": []
        }

        # è®¡ç®—å¹³å‡æµ‹è¯•æ—¶é—´
        if report['total_tests'] > 0:
            report['avg_test_duration'] = report['total_duration'] / report['total_tests']

        # è¯†åˆ«æœ€æ…¢çš„æµ‹è¯•ï¼ˆå¦‚æœæœ‰è¯¦ç»†æ•°æ®ï¼‰
        if 'tests' in self.results:
            tests = self.results['tests']
            sorted_tests = sorted(
                tests,
                key=lambda x: x.get('duration', 0),
                reverse=True
            )
            report['slowest_tests'] = [
                {
                    'name': t.get('name', 'unknown'),
                    'duration': t.get('duration', 0),
                    'type': t.get('type', 'unknown')
                }
                for t in sorted_tests[:10]
            ]

        # è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
        report['bottlenecks'] = self._identify_bottlenecks(report)

        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        report['recommendations'] = self._generate_recommendations(report)

        # è®¡ç®—æ€§èƒ½è¯„åˆ† (0-100)
        report['performance_score'] = self._calculate_performance_score(report)

        return report

    def _identify_bottlenecks(self, report: Dict) -> List[Dict]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []

        # ç“¶é¢ˆ1: æ€»æ‰§è¡Œæ—¶é—´è¿‡é•¿
        total_duration = report['total_duration']
        if total_duration > 1800:  # 30åˆ†é’Ÿ
            bottlenecks.append({
                "type": "long_total_duration",
                "severity": "high",
                "description": f"æ€»æ‰§è¡Œæ—¶é—´è¿‡é•¿: {total_duration:.1f}ç§’ (>{1800}ç§’)",
                "metric": total_duration
            })

        # ç“¶é¢ˆ2: å¹³å‡æµ‹è¯•æ—¶é—´è¿‡é•¿
        avg_duration = report['avg_test_duration']
        if avg_duration > 30:  # 30ç§’
            bottlenecks.append({
                "type": "slow_average_test",
                "severity": "medium",
                "description": f"å¹³å‡æµ‹è¯•æ—¶é—´è¿‡é•¿: {avg_duration:.1f}ç§’/æµ‹è¯•",
                "metric": avg_duration
            })

        # ç“¶é¢ˆ3: å­˜åœ¨è¶…æ…¢æµ‹è¯•
        if report['slowest_tests']:
            slowest = report['slowest_tests'][0]
            if slowest['duration'] > 60:  # 1åˆ†é’Ÿ
                bottlenecks.append({
                    "type": "very_slow_test",
                    "severity": "high",
                    "description": f"å­˜åœ¨è¶…æ…¢æµ‹è¯•: {slowest['name']} ({slowest['duration']:.1f}ç§’)",
                    "metric": slowest['duration'],
                    "test_name": slowest['name']
                })

        # ç“¶é¢ˆ4: E2Eæµ‹è¯•å æ¯”è¿‡é«˜
        if report['slowest_tests']:
            e2e_count = sum(
                1 for t in report['slowest_tests'][:10]
                if 'e2e' in t.get('type', '').lower()
            )
            if e2e_count > 7:  # å‰10ä¸ªæ…¢æµ‹è¯•ä¸­æœ‰7ä¸ªä»¥ä¸Šæ˜¯E2E
                bottlenecks.append({
                    "type": "too_many_e2e_tests",
                    "severity": "medium",
                    "description": f"E2Eæµ‹è¯•å æ¯”è¿‡é«˜: {e2e_count}/10 æœ€æ…¢æµ‹è¯•",
                    "metric": e2e_count
                })

        return bottlenecks

    def _generate_recommendations(self, report: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        bottleneck_types = {b['type'] for b in report['bottlenecks']}

        if 'long_total_duration' in bottleneck_types:
            recommendations.append(
                "ğŸš€ å¢åŠ å¹¶è¡Œæµ‹è¯• worker æ•°é‡: pytest -n auto (è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°)"
            )
            recommendations.append(
                "ğŸ“¦ å°†æµ‹è¯•æŒ‰ç±»å‹åˆ†ç»„è¿è¡Œ: å…ˆè¿è¡Œå•å…ƒæµ‹è¯•ï¼Œå†è¿è¡Œé›†æˆ/E2Eæµ‹è¯•"
            )

        if 'slow_average_test' in bottleneck_types:
            recommendations.append(
                "âš¡ ä¼˜åŒ–æµ‹è¯•ä¸­çš„ç­‰å¾…æ—¶é—´ï¼Œä½¿ç”¨æ™ºèƒ½ç­‰å¾…è€Œéå›ºå®š sleep"
            )
            recommendations.append(
                "ğŸ”§ æ£€æŸ¥æ˜¯å¦æœ‰ä¸å¿…è¦çš„é¡µé¢åŠ è½½ï¼Œè€ƒè™‘ä½¿ç”¨ API æµ‹è¯•ä»£æ›¿éƒ¨åˆ† E2E æµ‹è¯•"
            )

        if 'very_slow_test' in bottleneck_types:
            for b in report['bottlenecks']:
                if b['type'] == 'very_slow_test':
                    recommendations.append(
                        f"ğŸŒ é‡æ„è¶…æ…¢æµ‹è¯•: {b.get('test_name', 'unknown')} "
                        f"({b['metric']:.1f}ç§’ â†’ ç›®æ ‡ <30ç§’)"
                    )

        if 'too_many_e2e_tests' in bottleneck_types:
            recommendations.append(
                "ğŸ¯ å‡å°‘E2Eæµ‹è¯•è¦†ç›–èŒƒå›´ï¼Œä»…æµ‹è¯•å…³é”®è·¯å¾„ï¼ˆHappy Pathï¼‰"
            )
            recommendations.append(
                "ğŸ”„ å°†éƒ¨åˆ†E2Eæµ‹è¯•è½¬æ¢ä¸ºé›†æˆæµ‹è¯•æˆ–APIæµ‹è¯•"
            )

        # é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("âœ… æµ‹è¯•æ€§èƒ½è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰ä¼˜åŒ–ç­–ç•¥")
            recommendations.append("ğŸ’¡ è€ƒè™‘å¯ç”¨æµ‹è¯•ç¼“å­˜: pytest --cache-show")

        # æ€»æ˜¯æä¾›çš„å»ºè®®
        recommendations.append(
            "ğŸ“Š å®šæœŸè¿è¡Œæ€§èƒ½åˆ†æ: python scripts/analyze_performance.py"
        )

        return recommendations

    def _calculate_performance_score(self, report: Dict) -> int:
        """
        è®¡ç®—æ€§èƒ½è¯„åˆ† (0-100)

        è¯„åˆ†æ ‡å‡†:
        - æ€»æ—¶é—´ < 10åˆ†é’Ÿ: 40åˆ†
        - å¹³å‡æ—¶é—´ < 10ç§’: 30åˆ†
        - æ— ä¸¥é‡ç“¶é¢ˆ: 30åˆ†
        """
        score = 100

        # æ ¹æ®æ€»æ—¶é—´æ‰£åˆ†
        total_duration = report['total_duration']
        if total_duration > 600:  # 10åˆ†é’Ÿ
            score -= min(40, (total_duration - 600) / 30)  # æ¯è¶…è¿‡30ç§’æ‰£1åˆ†

        # æ ¹æ®å¹³å‡æ—¶é—´æ‰£åˆ†
        avg_duration = report['avg_test_duration']
        if avg_duration > 10:
            score -= min(30, (avg_duration - 10) * 2)  # æ¯è¶…è¿‡1ç§’æ‰£2åˆ†

        # æ ¹æ®ç“¶é¢ˆæ•°é‡å’Œä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        for bottleneck in report['bottlenecks']:
            if bottleneck['severity'] == 'high':
                score -= 15
            elif bottleneck['severity'] == 'medium':
                score -= 10
            else:
                score -= 5

        return max(0, int(score))

    def print_report(self, report: Dict):
        """æ‰“å°æ€§èƒ½åˆ†ææŠ¥å‘Š"""
        print("\nâš¡ æµ‹è¯•æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("=" * 60)

        # æ€§èƒ½è¯„åˆ†
        score = report['performance_score']
        if score >= 80:
            score_emoji = "ğŸŸ¢"
            score_text = "ä¼˜ç§€"
        elif score >= 60:
            score_emoji = "ğŸŸ¡"
            score_text = "è‰¯å¥½"
        elif score >= 40:
            score_emoji = "ğŸŸ "
            score_text = "ä¸€èˆ¬"
        else:
            score_emoji = "ğŸ”´"
            score_text = "éœ€ä¼˜åŒ–"

        print(f"\n{score_emoji} æ€§èƒ½è¯„åˆ†: {score}/100 ({score_text})")

        # åŸºæœ¬ç»Ÿè®¡
        print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"  æ€»æµ‹è¯•æ•°: {report['total_tests']}")
        print(f"  æ€»æ‰§è¡Œæ—¶é—´: {report['total_duration']:.1f}ç§’ ({report['total_duration']/60:.1f}åˆ†é’Ÿ)")
        print(f"  å¹³å‡æµ‹è¯•æ—¶é—´: {report['avg_test_duration']:.2f}ç§’/æµ‹è¯•")

        # æœ€æ…¢çš„æµ‹è¯•
        if report['slowest_tests']:
            print(f"\nğŸŒ æœ€æ…¢çš„10ä¸ªæµ‹è¯•:")
            for i, test in enumerate(report['slowest_tests'][:10], 1):
                print(f"  {i}. {test['name']}: {test['duration']:.1f}ç§’")

        # æ€§èƒ½ç“¶é¢ˆ
        if report['bottlenecks']:
            print(f"\nâš ï¸  æ€§èƒ½ç“¶é¢ˆ:")
            for i, bottleneck in enumerate(report['bottlenecks'], 1):
                severity_icons = {
                    "high": "ğŸ”´",
                    "medium": "ğŸŸ¡",
                    "low": "ğŸŸ¢"
                }
                icon = severity_icons.get(bottleneck['severity'], "âšª")
                print(f"  {i}. {icon} [{bottleneck['severity'].upper()}] {bottleneck['description']}")

        # ä¼˜åŒ–å»ºè®®
        if report['recommendations']:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(report['recommendations'], 1):
                print(f"  {i}. {rec}")

        print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æµ‹è¯•æ€§èƒ½åˆ†æ')
    parser.add_argument(
        '--results-file',
        default='reports/test-results.json',
        help='æµ‹è¯•ç»“æœæ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--json',
        action='store_true',
        help='è¾“å‡º JSON æ ¼å¼'
    )

    args = parser.parse_args()

    analyzer = PerformanceAnalyzer(results_file=args.results_file)
    report = analyzer.analyze()

    if args.json:
        print(json.dumps(report, indent=2))
    else:
        analyzer.print_report(report)

    # æ ¹æ®æ€§èƒ½è¯„åˆ†è¿”å›é€€å‡ºç 
    score = report.get('performance_score', 0)
    if score < 40:
        sys.exit(2)  # æ€§èƒ½å·®
    elif score < 60:
        sys.exit(1)  # æ€§èƒ½ä¸€èˆ¬
    else:
        sys.exit(0)  # æ€§èƒ½è‰¯å¥½


if __name__ == '__main__':
    main()
