#!/usr/bin/env python3
"""
å•†å“å‘ç°å‘½ä»¤è¡Œå·¥å…·

ä» Fiido ç½‘ç«™è‡ªåŠ¨å‘ç°å’ŒæŠ“å–å•†å“ä¿¡æ¯ï¼Œä¿å­˜ä¸º JSON æ–‡ä»¶ä¾›æµ‹è¯•ä½¿ç”¨ã€‚

ä½¿ç”¨ç¤ºä¾‹:
    # å‘ç°æ‰€æœ‰å•†å“
    python scripts/discover_products.py

    # åªå‘ç°ç‰¹å®šåˆ†ç±»
    python scripts/discover_products.py --collections bikes,accessories

    # é™åˆ¶æ¯ä¸ªåˆ†ç±»çš„å•†å“æ•°é‡
    python scripts/discover_products.py --limit 10

    # å¢é‡æ›´æ–°ï¼ˆåªæŠ“å–æ–°å•†å“ï¼‰
    python scripts/discover_products.py --incremental

    # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
    python scripts/discover_products.py --output data/my_products.json
"""

import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.crawler import ProductCrawler
from core.models import Product


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)


def load_existing_products(file_path: Path) -> Dict[str, Dict[str, Any]]:
    """åŠ è½½å·²å­˜åœ¨çš„å•†å“æ•°æ®

    Args:
        file_path: å•†å“æ•°æ®æ–‡ä»¶è·¯å¾„

    Returns:
        å•†å“å­—å…¸ï¼Œkey ä¸ºå•†å“ ID
    """
    if not file_path.exists():
        return {}

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return {p['id']: p for p in data.get('products', [])}
    except Exception as e:
        logger.warning(f"Failed to load existing products: {e}")
        return {}


def save_products(products: List[Product], file_path: Path, metadata: Dict[str, Any]):
    """ä¿å­˜å•†å“æ•°æ®åˆ° JSON æ–‡ä»¶

    Args:
        products: å•†å“åˆ—è¡¨
        file_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        metadata: å…ƒæ•°æ®ä¿¡æ¯
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    file_path.parent.mkdir(parents=True, exist_ok=True)

    # å°† Pydantic æ¨¡å‹è½¬æ¢ä¸ºå­—å…¸
    products_data = [p.model_dump(mode='json') for p in products]

    output = {
        'metadata': metadata,
        'products': products_data
    }

    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    logger.info(f"Products saved to {file_path}")


def print_statistics(
    total_collections: int,
    total_products: int,
    new_products: int,
    updated_products: int,
    duration: float
):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯

    Args:
        total_collections: æ‰«æçš„åˆ†ç±»æ€»æ•°
        total_products: å•†å“æ€»æ•°
        new_products: æ–°å¢å•†å“æ•°
        updated_products: æ›´æ–°å•†å“æ•°
        duration: æ‰§è¡Œè€—æ—¶ï¼ˆç§’ï¼‰
    """
    print("\n" + "=" * 60)
    print("å•†å“å‘ç°ç»Ÿè®¡")
    print("=" * 60)
    print(f"æ‰«æåˆ†ç±»æ•°: {total_collections}")
    print(f"å•†å“æ€»æ•°: {total_products}")
    print(f"æ–°å¢å•†å“: {new_products}")
    print(f"æ›´æ–°å•†å“: {updated_products}")
    print(f"æ‰§è¡Œè€—æ—¶: {duration:.2f} ç§’")
    print("=" * 60)


def discover_products_from_collections(
    crawler: ProductCrawler,
    collection_paths: List[str],
    limit_per_collection: int = None,
    existing_products: Dict[str, Dict[str, Any]] = None
) -> tuple[List[Product], int, int]:
    """ä»åˆ†ç±»åˆ—è¡¨ä¸­å‘ç°å•†å“

    Args:
        crawler: ProductCrawler å®ä¾‹
        collection_paths: åˆ†ç±»è·¯å¾„åˆ—è¡¨
        limit_per_collection: æ¯ä¸ªåˆ†ç±»çš„å•†å“æ•°é‡é™åˆ¶
        existing_products: å·²å­˜åœ¨çš„å•†å“å­—å…¸

    Returns:
        (å•†å“åˆ—è¡¨, æ–°å¢æ•°é‡, æ›´æ–°æ•°é‡)
    """
    # ä½¿ç”¨å­—å…¸è¿›è¡Œå»é‡ï¼Œkeyä¸ºå•†å“ID
    products_dict = {}
    new_count = 0
    updated_count = 0
    duplicate_count = 0
    existing_products = existing_products or {}

    for i, collection_path in enumerate(collection_paths, 1):
        logger.info(f"[{i}/{len(collection_paths)}] Processing collection: {collection_path}")

        try:
            products = crawler.discover_products(collection_path, limit=limit_per_collection)

            for product in products:
                # å»é‡æ£€æŸ¥ï¼šå¦‚æœå•†å“IDå·²å­˜åœ¨ï¼Œè·³è¿‡
                if product.id in products_dict:
                    duplicate_count += 1
                    logger.debug(f"  Skipping duplicate product: {product.name} (ID: {product.id})")
                    continue

                # æ£€æŸ¥æ˜¯æ–°å•†å“è¿˜æ˜¯æ›´æ–°
                if product.id in existing_products:
                    # æ›´æ–°å·²å­˜åœ¨çš„å•†å“
                    existing_product = existing_products[product.id]
                    # ä¿ç•™åŸæœ‰çš„æµ‹è¯•çŠ¶æ€å’Œæœ€åæµ‹è¯•æ—¶é—´
                    if 'test_status' in existing_product:
                        product.test_status = existing_product['test_status']
                    if 'last_tested' in existing_product and existing_product['last_tested']:
                        # ä¸è¦†ç›– last_testedï¼Œä¿æŒåŸå€¼
                        pass
                    updated_count += 1
                else:
                    # æ–°å•†å“
                    new_count += 1

                # æ·»åŠ åˆ°å»é‡å­—å…¸
                products_dict[product.id] = product

            logger.info(f"  Found {len(products)} products in {collection_path}")

        except Exception as e:
            logger.error(f"  Failed to process {collection_path}: {e}")
            continue

    # è½¬æ¢ä¸ºåˆ—è¡¨
    all_products = list(products_dict.values())

    if duplicate_count > 0:
        logger.info(f"ğŸ“Š De-duplication: Removed {duplicate_count} duplicate products")

    return all_products, new_count, updated_count


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ä» Fiido ç½‘ç«™å‘ç°å’ŒæŠ“å–å•†å“ä¿¡æ¯',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument(
        '--base-url',
        default='https://fiido.com',
        help='ç½‘ç«™æ ¹ URL (é»˜è®¤: https://fiido.com)'
    )

    parser.add_argument(
        '--collections',
        type=str,
        help='æŒ‡å®šè¦æ‰«æçš„åˆ†ç±»ï¼Œç”¨é€—å·åˆ†éš” (ä¾‹å¦‚: bikes,accessories)ã€‚ä¸æŒ‡å®šåˆ™æ‰«ææ‰€æœ‰åˆ†ç±»'
    )

    parser.add_argument(
        '--limit',
        type=int,
        help='é™åˆ¶æ¯ä¸ªåˆ†ç±»çš„å•†å“æ•°é‡'
    )

    parser.add_argument(
        '--output',
        type=Path,
        default=Path('data/products.json'),
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/products.json)'
    )

    parser.add_argument(
        '--incremental',
        action='store_true',
        help='å¢é‡æ›´æ–°æ¨¡å¼ï¼šåªæŠ“å–æ–°å•†å“ï¼Œä¿ç•™å·²æœ‰å•†å“çš„æµ‹è¯•çŠ¶æ€'
    )

    parser.add_argument(
        '--timeout',
        type=int,
        default=30,
        help='è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰(é»˜è®¤: 30)'
    )

    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = datetime.now()

    # åˆå§‹åŒ–çˆ¬è™«
    logger.info(f"Initializing ProductCrawler for {args.base_url}")
    crawler = ProductCrawler(base_url=args.base_url, timeout=args.timeout)

    try:
        # ç¡®å®šè¦æ‰«æçš„åˆ†ç±»
        if args.collections:
            # ç”¨æˆ·æŒ‡å®šåˆ†ç±»
            collection_paths = [f'/collections/{c.strip()}' for c in args.collections.split(',')]
            logger.info(f"Scanning specified collections: {collection_paths}")
        else:
            # è‡ªåŠ¨å‘ç°æ‰€æœ‰åˆ†ç±»
            logger.info("Discovering all collections...")
            collection_paths = crawler.discover_collections()
            logger.info(f"Found {len(collection_paths)} collections")

        if not collection_paths:
            logger.warning("No collections found!")
            return

        # åŠ è½½å·²å­˜åœ¨çš„å•†å“ï¼ˆå¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼‰
        existing_products = {}
        if args.incremental:
            logger.info(f"Loading existing products from {args.output}")
            existing_products = load_existing_products(args.output)
            logger.info(f"Loaded {len(existing_products)} existing products")

        # ä»åˆ†ç±»ä¸­å‘ç°å•†å“
        logger.info(f"Starting product discovery from {len(collection_paths)} collections...")
        all_products, new_count, updated_count = discover_products_from_collections(
            crawler=crawler,
            collection_paths=collection_paths,
            limit_per_collection=args.limit,
            existing_products=existing_products if args.incremental else None
        )

        # ä¿å­˜ç»“æœ
        metadata = {
            'base_url': args.base_url,
            'discovered_at': datetime.now().isoformat(),
            'total_collections': len(collection_paths),
            'total_products': len(all_products),
            'new_products': new_count,
            'updated_products': updated_count,
            'incremental': args.incremental
        }

        save_products(all_products, args.output, metadata)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        duration = (datetime.now() - start_time).total_seconds()
        print_statistics(
            total_collections=len(collection_paths),
            total_products=len(all_products),
            new_products=new_count,
            updated_products=updated_count,
            duration=duration
        )

        logger.info("Product discovery completed successfully!")

    except KeyboardInterrupt:
        logger.warning("Product discovery interrupted by user")
        sys.exit(1)

    except Exception as e:
        logger.error(f"Product discovery failed: {e}", exc_info=True)
        sys.exit(1)

    finally:
        crawler.close()


if __name__ == '__main__':
    main()
